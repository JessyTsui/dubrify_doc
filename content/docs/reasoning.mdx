---
title: 推理模型使用指南
description: OpenAI、Claude、Gemini 推理模型的详细配置说明，包含模型名称、参数、调用方式和完整代码示例
---

import { Tabs, Tab } from 'fumadocs-ui/components/tabs';
import { Callout } from 'fumadocs-ui/components/callout';
import { Card, Cards } from 'fumadocs-ui/components/card';

# 推理模型使用指南

推理模型（Reasoning Model）能够在回答前进行深度思考，适用于复杂的逻辑推理、数学计算、代码分析等场景。Dubrify API 支持 OpenAI、Claude、Gemini 三大厂商的推理模型，均通过 OpenAI 兼容格式调用。

---

## Claude 推理模型

Claude 推理模型默认**关闭推理功能**，需要手动启用。Dubrify 提供两种方式启用 Claude 的 Extended Thinking（扩展思考）能力。

Claude 系列模型均支持推理调用，包括 `claude-sonnet-4-6`、`claude-sonnet-4-5`、`claude-opus-4-6`、`claude-haiku-4-5` 等。如有新模型上线，一般也会同步支持，具体可自行测试验证。

### 方式一：模型名添加 `#thinking` 后缀

最简单的方式是在模型名后添加 `#thinking`，系统会自动启用推理并分配 80% 的 `max_tokens` 作为推理 token。

<Tabs items={['Python', 'cURL', 'JavaScript']}>
  <Tab value="Python">
```python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.dubrify.com/v1",
    api_key="sk-your-api-key"
)

response = client.chat.completions.create(
    model="claude-sonnet-4-6#thinking",  # 添加 #thinking 后缀
    max_tokens=4096,
    messages=[
        {"role": "user", "content": "strawberry 这个单词里有几个字母 r？"}
    ]
)

# 输出推理过程
print("=== 推理过程 ===")
print(response.choices[0].message.reasoning_content)
# 输出最终回复
print("=== 最终回复 ===")
print(response.choices[0].message.content)
```
  </Tab>
  <Tab value="cURL">
```bash
curl -X POST https://api.dubrify.com/v1/chat/completions \
  -H 'Authorization: Bearer sk-your-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "claude-sonnet-4-6#thinking",
    "max_tokens": 4096,
    "messages": [
      {"role": "user", "content": "strawberry 这个单词里有几个字母 r？"}
    ]
  }'
```
  </Tab>
  <Tab value="JavaScript">
```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'https://api.dubrify.com/v1',
  apiKey: 'sk-your-api-key'
});

const response = await client.chat.completions.create({
  model: 'claude-sonnet-4-6#thinking',  // 添加 #thinking 后缀
  max_tokens: 4096,
  messages: [
    { role: 'user', content: 'strawberry 这个单词里有几个字母 r？' }
  ]
});

// 输出推理过程
console.log('=== 推理过程 ===');
console.log(response.choices[0].message.reasoning_content);
// 输出最终回复
console.log('=== 最终回复 ===');
console.log(response.choices[0].message.content);
```
  </Tab>
</Tabs>

<Callout type="info">
使用 `#thinking` 后缀时，系统自动将 `max_tokens` 的 80% 分配给推理过程。例如 `max_tokens=4096` 时，推理 token 约为 3277。
</Callout>

### 方式二：使用 `reasoning` 参数

通过 `reasoning` 参数可以精确控制推理强度和 token 分配，适合需要细粒度控制的场景。

<Tabs items={['Python', 'cURL', 'JavaScript']}>
  <Tab value="Python">
```python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.dubrify.com/v1",
    api_key="sk-your-api-key"
)

# 使用 effort 控制推理强度
response = client.chat.completions.create(
    model="claude-sonnet-4-6",
    max_tokens=8192,
    messages=[
        {"role": "user", "content": "一个房间里有3个开关，分别控制另一个房间的3盏灯，你只能进入灯的房间一次，如何确定每个开关对应哪盏灯？"}
    ],
    reasoning={
        "effort": "high"  # 可选: low, medium, high
    }
)

# 输出推理过程
print("=== 推理过程 ===")
print(response.choices[0].message.reasoning_content)
# 输出最终回复
print("=== 最终回复 ===")
print(response.choices[0].message.content)
```
  </Tab>
  <Tab value="cURL">
```bash
curl -X POST https://api.dubrify.com/v1/chat/completions \
  -H 'Authorization: Bearer sk-your-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "claude-sonnet-4-6",
    "max_tokens": 8192,
    "messages": [
      {"role": "user", "content": "一个房间里有3个开关，分别控制另一个房间的3盏灯，你只能进入灯的房间一次，如何确定每个开关对应哪盏灯？"}
    ],
    "reasoning": {
      "effort": "high"
    }
  }'
```
  </Tab>
  <Tab value="JavaScript">
```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'https://api.dubrify.com/v1',
  apiKey: 'sk-your-api-key'
});

const response = await client.chat.completions.create({
  model: 'claude-sonnet-4-6',
  max_tokens: 8192,
  messages: [
    { role: 'user', content: '一个房间里有3个开关，分别控制另一个房间的3盏灯，你只能进入灯的房间一次，如何确定每个开关对应哪盏灯？' }
  ],
  reasoning: {
    effort: 'high'  // 可选: low, medium, high
  }
});

// 输出推理过程
console.log('=== 推理过程 ===');
console.log(response.choices[0].message.reasoning_content);
// 输出最终回复
console.log('=== 最终回复 ===');
console.log(response.choices[0].message.content);
```
  </Tab>
</Tabs>

#### 指定推理 token 数量

你也可以直接通过 `reasoning.max_tokens` 精确指定分配给推理过程的 token 数量：

```python
response = client.chat.completions.create(
    model="claude-opus-4-6",
    max_tokens=16384,
    messages=[
        {"role": "user", "content": "请深度分析这个商业方案的可行性..."}
    ],
    reasoning={
        "max_tokens": 10000  # 精确指定推理 token 数量
    }
)
```

### `reasoning` 参数说明

| 参数 | 类型 | 说明 |
|------|------|------|
| `effort` | string | 推理强度：`low`（20% max_tokens）、`medium`（50%）、`high`（80%） |
| `max_tokens` | integer | 推理专用 token 数量，不低于 1024 |

<Callout type="info">
如果同时设置了 `effort` 和 `max_tokens`，`max_tokens` 优先级更高。
</Callout>

### 两种方式对比

| 特性 | `#thinking` 后缀 | `reasoning` 参数 |
|------|------------------|------------------|
| 使用难度 | 简单，改模型名即可 | 需添加参数 |
| 推理强度控制 | 固定 80% | 可选 low/medium/high |
| 精确 token 控制 | 不支持 | 支持指定 `max_tokens` |
| 适用场景 | 快速启用推理 | 需要细粒度控制 |

### 流式请求（Streaming）

推理模型响应时间较长，推荐使用流式请求实时获取推理过程和回复内容。

<Tabs items={['Python', 'cURL', 'JavaScript']}>
  <Tab value="Python">
```python
from openai import OpenAI

client = OpenAI(
    base_url="https://api.dubrify.com/v1",
    api_key="sk-your-api-key"
)

stream = client.chat.completions.create(
    model="claude-sonnet-4-6#thinking",
    max_tokens=4096,
    stream=True,
    messages=[
        {"role": "user", "content": "strawberry 这个单词里有几个字母 r？"}
    ]
)

for chunk in stream:
    if not chunk.choices:
        continue
    delta = chunk.choices[0].delta
    # 推理过程
    if hasattr(delta, "reasoning_content") and delta.reasoning_content:
        print(delta.reasoning_content, end="", flush=True)
    # 最终回复
    if delta.content:
        print(delta.content, end="", flush=True)
```
  </Tab>
  <Tab value="cURL">
```bash
curl -X POST https://api.dubrify.com/v1/chat/completions \
  -H 'Authorization: Bearer sk-your-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "claude-sonnet-4-6#thinking",
    "max_tokens": 4096,
    "stream": true,
    "messages": [
      {"role": "user", "content": "strawberry 这个单词里有几个字母 r？"}
    ]
  }'
```
  </Tab>
  <Tab value="JavaScript">
```javascript
import OpenAI from 'openai';

const client = new OpenAI({
  baseURL: 'https://api.dubrify.com/v1',
  apiKey: 'sk-your-api-key'
});

const stream = await client.chat.completions.create({
  model: 'claude-sonnet-4-6#thinking',
  max_tokens: 4096,
  stream: true,
  messages: [
    { role: 'user', content: 'strawberry 这个单词里有几个字母 r？' }
  ]
});

for await (const chunk of stream) {
  if (!chunk.choices.length) continue;
  const delta = chunk.choices[0].delta;
  // 推理过程
  if (delta.reasoning_content) {
    process.stdout.write(delta.reasoning_content);
  }
  // 最终回复
  if (delta.content) {
    process.stdout.write(delta.content);
  }
}
```
  </Tab>
</Tabs>

<Callout type="info">
流式响应中，推理内容通过 `delta.reasoning_content` 字段返回，最终回复通过 `delta.content` 返回，两者按顺序输出。`reasoning` 参数方式同样支持流式请求，添加 `stream=True` 即可。
</Callout>

### 注意事项

<Callout type="warning">
**参数限制**: 启用推理后，`temperature` 参数只能设为 1，不支持自定义温度值。
</Callout>

- 未指定 `max_tokens` 时，系统使用 Claude 模型允许的最大 token 数（如 claude-sonnet-4-6 为 128,000）
- 推理模型响应时间较长，建议使用流式请求提升体验
- 推理过程的 token 也会计入用量计费

---

## OpenAI 推理模型

针对 `o3`、`o4-mini` 等推理模型的特殊配置。

### 支持的模型

| 模型 | 说明 |
|------|------|
| `o3` | 强推理能力 |
| `o4-mini` | 轻量推理，速度更快 |

### 默认行为
- 自动启用推理，默认开启，**无法手动关闭**
- 不支持 `temperature`、`top_k`、`top_p` 等参数
- 内部推理过程不返回给用户

### 推理强度控制

<Tabs items={['Python', 'cURL']}>
  <Tab value="Python">
```python
response = client.chat.completions.create(
    model="o3",
    messages=[{"role": "user", "content": "解决这个复杂的数学问题..."}],
    reasoning={
        "effort": "high"  # 可选: low, medium, high
    }
)
```
  </Tab>
  <Tab value="cURL">
```bash
curl -X POST https://api.dubrify.com/v1/chat/completions \
  -H 'Authorization: Bearer sk-your-api-key' \
  -H 'Content-Type: application/json' \
  -d '{
    "model": "o3",
    "messages": [
      {"role": "user", "content": "解决这个复杂的数学问题..."}
    ],
    "reasoning": {
      "effort": "high"
    }
  }'
```
  </Tab>
</Tabs>

<Callout type="info">
推理模型响应时间较长，推荐设置较大的 `max_tokens` 值。如返回 400 错误，请检查提示词是否符合使用规范。
</Callout>

---

## Gemini 推理模型

### 支持的模型

| 模型 | 说明 | 推理默认状态 |
|------|------|-------------|
| `gemini-2.5-flash` | 快速推理模型 | 默认开启，可关闭 |
| `gemini-2.5-pro` | 高性能推理模型 | 默认开启，不可关闭 |

### Gemini 2.5-flash

```python
response = client.chat.completions.create(
    model="gemini-2.5-flash",
    messages=[{"role": "user", "content": "深度分析..."}],
    reasoning={
        "max_tokens": 2048  # 最低 1024，设为 0 可关闭推理
    }
)
```

### Gemini 2.5-pro

```python
response = client.chat.completions.create(
    model="gemini-2.5-pro",
    messages=[{"role": "user", "content": "复杂推理任务..."}],
    reasoning={
        "max_tokens": 4096  # 默认开启，不可关闭，最低 1024
    }
)
```

---

## 各厂商推理模型对比

| 特性 | OpenAI (o3/o4-mini) | Claude | Gemini |
|------|---------------------|--------|--------|
| 推理默认状态 | 始终开启 | 默认关闭 | 默认开启 |
| 关闭推理 | 不支持 | 不传 reasoning 参数 | 仅 flash 支持（设 0） |
| `effort` 参数 | 支持 | 支持 | 不支持 |
| `reasoning.max_tokens` | 不支持 | 支持 | 支持 |
| `#thinking` 后缀 | 不支持 | 支持 | 不支持 |
| `temperature` 限制 | 不可用 | 必须为 1 | 无限制 |
